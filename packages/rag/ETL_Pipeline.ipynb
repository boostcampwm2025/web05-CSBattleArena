{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline: PDF to PostgreSQL pgvector\n",
    "\n",
    "이 노트북은 PDF 파일을 읽어서 마크다운으로 변환하고, 청크로 나눈 후 임베딩하여 PostgreSQL pgvector에 적재하는 전체 ETL 파이프라인을 구현합니다.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Chapter-based Markdown Conversion**: 챕터 배열을 받아서 챕터별로 나눈 후 마크다운으로 변환\n",
    "2. **Noise Removal**: 챕터 첫 페이지와 전체 페이지에서 노이즈 제거 (header/footer 패턴)\n",
    "3. **Markdown Header Labeling**: 특정 패턴(Exercises, Key Terms 등)을 마크다운 헤더로 변환\n",
    "4. **Header-based Chunking**: 마크다운 헤더를 기준으로 청크 분할\n",
    "5. **Header-based Filtering**: 특정 헤더를 가진 청크 필터링\n",
    "6. **Embedding with Clova**: Clova 임베딩 API를 사용한 벡터 생성 (RPM 고려)\n",
    "7. **PostgreSQL pgvector Loading**: 임베딩 결과를 PostgreSQL pgvector에 적재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 1: Chapter-based Markdown Conversion\n",
    "\n",
    "PDF 파일에서 챕터 정보를 받아 각 챕터별로 페이지를 나누고 마크다운으로 변환합니다.\n",
    "\n",
    "### 입력\n",
    "- PDF 파일 경로\n",
    "- 챕터별 시작 페이지 정보 (딕셔너리)\n",
    "\n",
    "### 출력\n",
    "- `chapter_markdowns`: 챕터별 마크다운 텍스트 (dict)\n",
    "\n",
    "### 체크포인트 저장\n",
    "- 변수: `checkpoint_1_chapter_markdowns`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: 라이브러리 설치\n",
    "\n",
    "필요한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf4llm pymupdf -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: 라이브러리 임포트\n",
    "\n",
    "필요한 라이브러리를 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "import fitz  # PyMuPDF\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: PDF 파일 및 챕터 정보 설정\n",
    "\n",
    "처리할 PDF 파일 경로와 각 챕터의 시작 페이지 번호를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 경로\n",
    "pdf_path = \"data/network.pdf\"\n",
    "\n",
    "# 챕터별 시작 페이지 번호 (PDF 페이지 번호)\n",
    "chapter_start_pages = {\n",
    "    \"Chapter 1\": 5,\n",
    "    \"Chapter 2\": 47,\n",
    "    \"Chapter 3\": 103,\n",
    "    \"Chapter 4\": 179,\n",
    "    \"Chapter 5\": 229,\n",
    "    \"Chapter 6\": 287,\n",
    "    \"Chapter 7\": 341,\n",
    "    \"Chapter 8\": 373,\n",
    "    \"Chapter 9\": 415,\n",
    "}\n",
    "\n",
    "# 챕터 정보 정렬 및 총 페이지 수 확인\n",
    "sorted_chapters = sorted(chapter_start_pages.items(), key=lambda item: item[1])\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "total_pages = len(doc)\n",
    "doc.close()\n",
    "\n",
    "print(f\"Target PDF: {pdf_path}\")\n",
    "print(f\"Total Pages: {total_pages}\")\n",
    "print(f\"Total Chapters: {len(sorted_chapters)}\")\n",
    "print(f\"\\nChapter Configuration:\")\n",
    "for chapter_name, start_page in sorted_chapters:\n",
    "    print(f\"  - {chapter_name}: Page {start_page}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: 챕터별 마크다운 변환 실행\n",
    "\n",
    "각 챕터의 페이지 범위를 계산하고 pymupdf4llm을 사용하여 마크다운으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_markdowns = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Chapter-based Markdown Conversion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (chapter_name, start_page) in enumerate(sorted_chapters):\n",
    "    # 페이지 범위 계산 (0-indexed)\n",
    "    start_idx = start_page - 1\n",
    "    \n",
    "    if i < len(sorted_chapters) - 1:\n",
    "        # 다음 챕터 시작 전까지\n",
    "        end_idx = sorted_chapters[i + 1][1] - 1\n",
    "    else:\n",
    "        # 마지막 챕터는 PDF 끝까지\n",
    "        end_idx = total_pages\n",
    "    \n",
    "    # 페이지 범위 리스트 생성\n",
    "    page_range = list(range(start_idx, end_idx))\n",
    "    \n",
    "    print(f\"\\n[{chapter_name}]\")\n",
    "    print(f\"  Processing pages {start_idx + 1} to {end_idx} ({len(page_range)} pages)...\")\n",
    "    \n",
    "    # pymupdf4llm을 사용하여 페이지별로 마크다운 변환\n",
    "    chapter_md_text = pymupdf4llm.to_markdown(pdf_path, pages=page_range)\n",
    "    \n",
    "    # 챕터별 마크다운 저장\n",
    "    chapter_markdowns[chapter_name] = chapter_md_text\n",
    "    \n",
    "    print(f\"  ✓ Completed (Length: {len(chapter_md_text):,} characters)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ All chapters converted successfully!\")\n",
    "print(f\"  Total chapters: {len(chapter_markdowns)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: 체크포인트 저장 및 결과 확인\n",
    "\n",
    "변환된 마크다운을 체크포인트로 저장하고 샘플 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 1 저장\n",
    "checkpoint_1_chapter_markdowns = chapter_markdowns.copy()\n",
    "\n",
    "print(\"✓ Checkpoint 1 saved successfully!\")\n",
    "print(f\"  Variable: checkpoint_1_chapter_markdowns\")\n",
    "print(f\"  Chapters: {list(checkpoint_1_chapter_markdowns.keys())}\")\n",
    "print(f\"\\n--- Sample: First 500 characters of Chapter 1 ---\")\n",
    "print(checkpoint_1_chapter_markdowns[\"Chapter 1\"][:3000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 2: Noise Removal\n",
    "\n",
    "변환된 마크다운에서 불필요한 노이즈를 제거합니다.\n",
    "- 전체 페이지: Header/Footer 영역의 페이지 번호, Copyright 문구 등 제거\n",
    "- 챕터 첫 페이지: 추가로 중복되는 챕터 제목 등 제거\n",
    "\n",
    "### 입력\n",
    "- `checkpoint_1_chapter_markdowns`: 챕터별 원본 마크다운\n",
    "\n",
    "### 출력\n",
    "- `chapter_markdowns_cleaned`: 노이즈가 제거된 챕터별 마크다운 (dict)\n",
    "\n",
    "### 체크포인트 저장\n",
    "- 변수: `checkpoint_2_cleaned_markdowns`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: 라이브러리 임포트\n",
    "\n",
    "정규표현식 처리를 위한 라이브러리를 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: 노이즈 제거 패턴 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header/Footer 확인 범위 설정 (각각 몇 줄씩 체크할지)\n",
    "header_check_range = 5\n",
    "footer_check_range = 5\n",
    "\n",
    "# Header/Footer 무조건 삭제할 줄 수 (패턴 무관)\n",
    "header_remove_lines = 0     # 상단 N줄을 무조건 삭제 (0 = 삭제 안함)\n",
    "footer_remove_lines = 4     # 하단 N줄을 무조건 삭제 (0 = 삭제 안함)\n",
    "\n",
    "# Header 패턴 (페이지 상단에서 제거할 패턴)\n",
    "header_patterns = [\n",
    "    r\"Computer Networks: A Systems Approach, Release Version 6.1\"\n",
    "]\n",
    "\n",
    "# Footer 패턴 (페이지 하단에서 제거할 패턴)\n",
    "footer_patterns = []\n",
    "\n",
    "# 챕터 첫 페이지 전용 패턴 (첫 페이지에서만 추가로 제거)\n",
    "first_page_patterns = []\n",
    "\n",
    "print(\"Noise Removal Configuration:\")\n",
    "print(f\"  Header check range: {header_check_range} lines\")\n",
    "print(f\"  Header unconditional removal: {header_remove_lines} lines\")\n",
    "print(f\"  Footer check range: {footer_check_range} lines\")\n",
    "print(f\"  Footer unconditional removal: {footer_remove_lines} lines\")\n",
    "print(f\"  Header patterns: {len(header_patterns)} patterns\")\n",
    "print(f\"  Footer patterns: {len(footer_patterns)} patterns\")\n",
    "print(f\"  First page patterns: {len(first_page_patterns)} patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: 노이즈 제거 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page_markdown(md_text, is_first_page=False, show_removed=False, page_num=None):\n",
    "    \"\"\"\n",
    "    페이지별 마크다운에서 노이즈를 제거합니다.\n",
    "    \n",
    "    Args:\n",
    "        md_text (str): 원본 마크다운 텍스트\n",
    "        is_first_page (bool): 챕터의 첫 페이지 여부\n",
    "        show_removed (bool): 제거된 줄을 표시할지 여부\n",
    "        page_num (int): 페이지 번호 (로깅용)\n",
    "        \n",
    "    Returns:\n",
    "        str: 노이즈가 제거된 마크다운 텍스트\n",
    "    \"\"\"\n",
    "    if not md_text:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = md_text.split('\\n')\n",
    "    lines_to_remove = set()\n",
    "    removal_reasons = {}  # 제거 이유 저장\n",
    "    \n",
    "    # 1. 무조건 삭제할 줄 처리\n",
    "    # Header: 상단 N줄 무조건 삭제\n",
    "    if header_remove_lines > 0:\n",
    "        for i in range(min(header_remove_lines, len(lines))):\n",
    "            lines_to_remove.add(i)\n",
    "            removal_reasons[i] = f\"Header unconditional removal (top {header_remove_lines} lines)\"\n",
    "    \n",
    "    # Footer: 하단 N줄 무조건 삭제\n",
    "    if footer_remove_lines > 0:\n",
    "        footer_start = max(0, len(lines) - footer_remove_lines)\n",
    "        for i in range(footer_start, len(lines)):\n",
    "            lines_to_remove.add(i)\n",
    "            removal_reasons[i] = f\"Footer unconditional removal (bottom {footer_remove_lines} lines)\"\n",
    "    \n",
    "    # 2. Header 영역에서 패턴 매칭\n",
    "    for i in range(min(header_check_range, len(lines))):\n",
    "        if i in lines_to_remove:\n",
    "            continue  # 이미 제거 대상이면 스킵\n",
    "        \n",
    "        line_stripped = lines[i].strip()\n",
    "        \n",
    "        # Header 패턴 체크\n",
    "        for pattern in header_patterns:\n",
    "            if re.search(pattern, line_stripped, re.IGNORECASE):\n",
    "                lines_to_remove.add(i)\n",
    "                removal_reasons[i] = f\"Header pattern: {pattern}\"\n",
    "                break\n",
    "    \n",
    "    # 3. Footer 영역에서 패턴 매칭\n",
    "    footer_start_idx = max(0, len(lines) - footer_check_range)\n",
    "    for i in range(footer_start_idx, len(lines)):\n",
    "        if i in lines_to_remove:\n",
    "            continue  # 이미 제거 대상이면 스킵\n",
    "        \n",
    "        line_stripped = lines[i].strip()\n",
    "        \n",
    "        # Footer 패턴 체크\n",
    "        for pattern in footer_patterns:\n",
    "            if re.search(pattern, line_stripped, re.IGNORECASE):\n",
    "                lines_to_remove.add(i)\n",
    "                removal_reasons[i] = f\"Footer pattern: {pattern}\"\n",
    "                break\n",
    "    \n",
    "    # 4. 챕터 첫 페이지인 경우 추가 패턴 체크\n",
    "    if is_first_page:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i in lines_to_remove:\n",
    "                continue\n",
    "            line_stripped = line.strip()\n",
    "            for pattern in first_page_patterns:\n",
    "                if re.search(pattern, line_stripped, re.IGNORECASE):\n",
    "                    lines_to_remove.add(i)\n",
    "                    removal_reasons[i] = f\"First page pattern: {pattern}\"\n",
    "                    break\n",
    "    \n",
    "    # 제거된 줄 표시\n",
    "    if show_removed and lines_to_remove:\n",
    "        print(f\"    [Page {page_num}] Removed {len(lines_to_remove)} line(s):\")\n",
    "        for i in sorted(lines_to_remove)[:10]:  # 처음 10개만 표시\n",
    "            reason = removal_reasons.get(i, \"Unknown\")\n",
    "            line_preview = lines[i][:60] + \"...\" if len(lines[i]) > 60 else lines[i]\n",
    "            print(f\"      Line {i}: '{line_preview}' ({reason})\")\n",
    "        if len(lines_to_remove) > 10:\n",
    "            print(f\"      ... and {len(lines_to_remove) - 10} more lines\")\n",
    "    \n",
    "    # 제거 대상이 아닌 줄만 필터링\n",
    "    cleaned_lines = [line for i, line in enumerate(lines) if i not in lines_to_remove]\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "print(\"✓ Noise removal function defined successfully!\")\n",
    "print(\"  - Header: Unconditional removal + pattern matching\")\n",
    "print(\"  - Footer: Unconditional removal + pattern matching\")\n",
    "print(\"  - First page: Pattern matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: 챕터별 노이즈 제거 실행\n",
    "\n",
    "Checkpoint 1에서 변환된 마크다운에 노이즈 제거 함수를 적용합니다.\n",
    "각 챕터는 여러 페이지로 구성되어 있으므로, 페이지 단위로 분리하여 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_markdowns_cleaned = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Noise Removal\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (chapter_name, start_page) in enumerate(sorted_chapters):\n",
    "    # 페이지 범위 계산 (0-indexed)\n",
    "    start_idx = start_page - 1\n",
    "    \n",
    "    if i < len(sorted_chapters) - 1:\n",
    "        end_idx = sorted_chapters[i + 1][1] - 1\n",
    "    else:\n",
    "        end_idx = total_pages\n",
    "    \n",
    "    print(f\"\\n[{chapter_name}]\")\n",
    "    print(f\"  Processing pages {start_idx + 1} to {end_idx}...\")\n",
    "    \n",
    "    # 페이지별로 변환하고 노이즈 제거 적용\n",
    "    chapter_full_text = []\n",
    "    \n",
    "    for page_idx in range(start_idx, end_idx):\n",
    "        # 첫 페이지 여부 확인\n",
    "        is_first_page = (page_idx == start_idx)\n",
    "        \n",
    "        # 페이지를 마크다운으로 변환\n",
    "        page_md = pymupdf4llm.to_markdown(pdf_path, pages=[page_idx])\n",
    "        \n",
    "        # 노이즈 제거 적용 (제거된 줄 표시)\n",
    "        cleaned_md = clean_page_markdown(\n",
    "            page_md, \n",
    "            is_first_page=is_first_page, \n",
    "            show_removed=True, \n",
    "            page_num=page_idx + 1\n",
    "        )\n",
    "        \n",
    "        chapter_full_text.append(cleaned_md)\n",
    "    \n",
    "    # 챕터의 모든 페이지를 결합\n",
    "    chapter_markdowns_cleaned[chapter_name] = \"\\n\\n\".join(chapter_full_text)\n",
    "    \n",
    "    original_length = len(checkpoint_1_chapter_markdowns.get(chapter_name, \"\"))\n",
    "    cleaned_length = len(chapter_markdowns_cleaned[chapter_name])\n",
    "    removed = original_length - cleaned_length\n",
    "    \n",
    "    print(f\"  ✓ Completed\")\n",
    "    print(f\"    Original: {original_length:,} chars\")\n",
    "    print(f\"    Cleaned: {cleaned_length:,} chars\")\n",
    "    print(f\"    Removed: {removed:,} chars\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Noise removal completed for all chapters!\")\n",
    "print(f\"  Total chapters: {len(chapter_markdowns_cleaned)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: 체크포인트 저장 및 결과 확인\n",
    "\n",
    "노이즈가 제거된 마크다운을 체크포인트로 저장하고 샘플 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 2 저장\n",
    "checkpoint_2_cleaned_markdowns = chapter_markdowns_cleaned.copy()\n",
    "\n",
    "print(\"✓ Checkpoint 2 saved successfully!\")\n",
    "print(f\"  Variable: checkpoint_2_cleaned_markdowns\")\n",
    "print(f\"  Chapters: {list(checkpoint_2_cleaned_markdowns.keys())}\")\n",
    "print(f\"\\n--- Sample: First 500 characters of cleaned Chapter 1 ---\")\n",
    "print(checkpoint_2_cleaned_markdowns[\"Chapter 9\"][:1000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 3: Markdown Header Labeling\n",
    "\n",
    "특정 키워드(Exercises, Attribution, Key Terms 등)를 찾아서 마크다운 헤더로 변환합니다.\n",
    "\n",
    "### 입력\n",
    "- `checkpoint_2_cleaned_markdowns`: 정제된 챕터별 마크다운\n",
    "- `header_labels`: 변환할 키워드와 헤더 레벨 매핑 (dict)\n",
    "\n",
    "### 출력\n",
    "- `chapter_markdowns_labeled`: 헤더가 라벨링된 챕터별 마크다운 (dict)\n",
    "\n",
    "### 체크포인트 저장\n",
    "- 변수: `checkpoint_3_labeled_markdowns`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: 헤더 라벨링 설정\n",
    "\n",
    "특정 키워드를 마크다운 헤더로 변환하기 위한 설정을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헤더로 변환할 키워드와 헤더 레벨 매핑\n",
    "# 키: 찾을 키워드, 값: 헤더 레벨 (2 = ##)\n",
    "header_labels = {\n",
    "    \"Key Takeaway\": 1\n",
    "}\n",
    "\n",
    "print(\"Header Labeling Configuration:\")\n",
    "print(f\"  Total keywords: {len(header_labels)}\")\n",
    "print(f\"\\nKeywords to convert:\")\n",
    "for keyword, level in header_labels.items():\n",
    "    header_symbol = \"#\" * level\n",
    "    print(f\"  - '{keyword}' → '{header_symbol} {keyword}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: 헤더 라벨링 함수 정의\n",
    "\n",
    "키워드를 찾아서 마크다운 헤더로 변환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def label_headers(text, label_map, show_conversions=False):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    new_lines = []\n",
    "    conversions = []\n",
    "    \n",
    "    for line_num, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        matched = False\n",
    "        \n",
    "        # 1. 마크다운 특수 기호들 제거 (앞뒤에 붙는 것들 위주)\n",
    "        cleaned_line = line_stripped.strip(\"#* _~`>-\")\n",
    "        \n",
    "        # 2. HTML 태그가 섞여 있을 경우 제거\n",
    "        cleaned_line = re.sub(r'<[^>]*>', '', cleaned_line).strip()\n",
    "        \n",
    "        # 각 키워드에 대해 매칭 시도\n",
    "        for keyword, level in label_map.items():\n",
    "            if cleaned_line.lower() == keyword.lower():\n",
    "                prefix = \"#\" * level\n",
    "                new_line = f\"{prefix} {keyword}\"\n",
    "                new_lines.append(new_line)\n",
    "                matched = True\n",
    "                \n",
    "                if show_conversions:\n",
    "                    conversions.append({\n",
    "                        'line_num': line_num,\n",
    "                        'original': line_stripped,\n",
    "                        'converted': new_line,\n",
    "                        'keyword': keyword\n",
    "                    })\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            new_lines.append(line)\n",
    "    \n",
    "    if show_conversions and conversions:\n",
    "        print(f\"    Found {len(conversions)} header(s) to label:\")\n",
    "        for conv in conversions:\n",
    "            print(f\"      Line {conv['line_num']}: '{conv['original']}' → '{conv['converted']}'\")\n",
    "    \n",
    "    return '\\n'.join(new_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: 챕터별 헤더 라벨링 실행\n",
    "\n",
    "Checkpoint 2에서 정제된 마크다운에 헤더 라벨링을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_markdowns_labeled = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Header Labeling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_conversions = 0\n",
    "\n",
    "for chapter_name, text in checkpoint_2_cleaned_markdowns.items():\n",
    "    print(f\"\\n[{chapter_name}]\")\n",
    "    print(f\"  Processing header labeling...\")\n",
    "    \n",
    "    # 헤더 라벨링 적용 (변환 내용 표시)\n",
    "    labeled_text = label_headers(text, header_labels, show_conversions=True)\n",
    "    \n",
    "    # 변환된 텍스트 저장\n",
    "    chapter_markdowns_labeled[chapter_name] = labeled_text\n",
    "    \n",
    "    print(f\"  ✓ Completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Header labeling completed for all chapters!\")\n",
    "print(f\"  Total chapters: {len(chapter_markdowns_labeled)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: 체크포인트 저장 및 결과 확인\n",
    "\n",
    "헤더가 라벨링된 마크다운을 체크포인트로 저장하고 샘플 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 3 저장\n",
    "checkpoint_3_labeled_markdowns = chapter_markdowns_labeled.copy()\n",
    "\n",
    "print(\"✓ Checkpoint 3 saved successfully!\")\n",
    "print(f\"  Variable: checkpoint_3_labeled_markdowns\")\n",
    "print(f\"  Chapters: {list(checkpoint_3_labeled_markdowns.keys())}\")\n",
    "\n",
    "# 변환 예시 찾기\n",
    "sample_chapter = None\n",
    "for chapter_name, text in checkpoint_3_labeled_markdowns.items():\n",
    "    if \"## Key Terms\" in text or \"## Exercises\" in text:\n",
    "        sample_chapter = chapter_name\n",
    "        break\n",
    "\n",
    "if sample_chapter:\n",
    "    print(f\"\\n--- Sample: {sample_chapter} (showing Key Terms/Exercises section) ---\")\n",
    "    text = checkpoint_3_labeled_markdowns[sample_chapter]\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"## Key Terms\") or line.startswith(\"## Exercises\"):\n",
    "            sample_text = '\\n'.join(lines[i:min(i+10, len(lines))])\n",
    "            print(sample_text)\n",
    "            print(\"...\")\n",
    "            break\n",
    "else:\n",
    "    print(\"\\n--- Sample: First 500 characters of Chapter 1 ---\")\n",
    "    print(checkpoint_3_labeled_markdowns[\"Chapter 1\"][:500])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 4: Header-based Chunking\n",
    "\n",
    "LangChain의 MarkdownHeaderTextSplitter를 사용하여 마크다운 헤더를 기준으로 텍스트를 청크로 분할합니다.\n",
    "\n",
    "### 입력\n",
    "- `checkpoint_3_labeled_markdowns`: 헤더가 라벨링된 챕터별 마크다운\n",
    "- `headers_to_split_on`: 분할 기준이 되는 헤더 레벨 (list)\n",
    "\n",
    "### 출력\n",
    "- `chapter_chunks`: 챕터별 청크 리스트 (dict)\n",
    "\n",
    "### 체크포인트 저장\n",
    "- 변수: `checkpoint_4_chunks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: 라이브러리 설치\n",
    "\n",
    "LangChain의 MarkdownHeaderTextSplitter를 사용하기 위한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-text-splitters -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: MarkdownHeaderTextSplitter 및 RecursiveCharacterTextSplitter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# 분할 기준이 되는 헤더 레벨 설정\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "]\n",
    "\n",
    "# MarkdownHeaderTextSplitter 생성\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "# RecursiveCharacterTextSplitter 생성 (큰 청크 재분할용)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1600,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"MarkdownHeaderTextSplitter Configuration:\")\n",
    "print(f\"  Headers to split on:\")\n",
    "for header_symbol, metadata_key in headers_to_split_on:\n",
    "    print(f\"    - '{header_symbol}' → metadata key: '{metadata_key}'\")\n",
    "\n",
    "print(f\"\\nRecursiveCharacterTextSplitter Configuration:\")\n",
    "print(f\"  Chunk size: 1600 characters (~400 tokens)\")\n",
    "print(f\"  Chunk overlap: 200 characters (~50 tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: 청크 분할 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def split_large_chunk(chunk, threshold=700):\n",
    "    \"\"\"\n",
    "    700자를 초과하는 청크를 RecursiveCharacterTextSplitter로 재분할하고\n",
    "    각 서브청크에 원래 헤더 정보를 보존하며, 끊긴 문장을 병합합니다.\n",
    "    \"\"\"\n",
    "    if len(chunk.page_content) <= threshold:\n",
    "        return [chunk]\n",
    "\n",
    "    # 메타데이터에서 헤더 정보 추출\n",
    "    headers = []\n",
    "    for i in range(1, 6):\n",
    "        header_key = f\"Header {i}\"\n",
    "        if header_key in chunk.metadata:\n",
    "            header_symbols = \"#\" * i\n",
    "            headers.append(f\"{header_symbols} {chunk.metadata[header_key]}\")\n",
    "    \n",
    "    header_text = \"\\n\".join(headers) if headers else \"\"\n",
    "\n",
    "    # 본문만 추출\n",
    "    content = chunk.page_content\n",
    "    for header in headers:\n",
    "        content = content.replace(header, \"\").strip()\n",
    "\n",
    "    if not content:\n",
    "        return [chunk]\n",
    "\n",
    "    # 스마트 문단 감지 전처리\n",
    "    content = re.sub(r' {3,}', '\\n\\n', content)\n",
    "    content = re.sub(r'\\n([A-Z][a-z]{3,})', r'\\n\\n\\1', content)\n",
    "\n",
    "    # RecursiveCharacterTextSplitter로 재분할\n",
    "    sub_texts = recursive_splitter.split_text(content)\n",
    "\n",
    "    # 후처리: 소문자로 시작하는 청크 병합\n",
    "    merged_texts = []\n",
    "    for text in sub_texts:\n",
    "        text_stripped = text.strip()\n",
    "        if not text_stripped:\n",
    "            continue\n",
    "        \n",
    "        if text_stripped[0].islower() and merged_texts:\n",
    "            merged_texts[-1] = merged_texts[-1] + \" \" + text_stripped\n",
    "        else:\n",
    "            merged_texts.append(text_stripped)\n",
    "\n",
    "    # Document 생성\n",
    "    sub_chunks = []\n",
    "    for sub_text in merged_texts:\n",
    "        full_content = f\"{header_text}\\n\\n{sub_text}\" if header_text else sub_text\n",
    "        \n",
    "        sub_chunk = Document(\n",
    "            page_content=full_content,\n",
    "            metadata=chunk.metadata.copy()\n",
    "        )\n",
    "        sub_chunks.append(sub_chunk)\n",
    "        \n",
    "    return sub_chunks\n",
    "\n",
    "# 메인 실행\n",
    "chapter_chunks = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Header-based Chunking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_chunks = 0\n",
    "\n",
    "for chapter_name, text in checkpoint_3_labeled_markdowns.items():\n",
    "    print(f\"\\n[{chapter_name}]\")\n",
    "    print(f\"  Text length: {len(text):,}\")\n",
    "    \n",
    "    # MarkdownHeaderTextSplitter로 1차 분할\n",
    "    initial_chunks = markdown_splitter.split_text(text)\n",
    "    print(f\"  ✓ Initial split: {len(initial_chunks)} chunk(s)\")\n",
    "    \n",
    "    # 크기 임계값 초과 시 재분할\n",
    "    final_chunks = []\n",
    "    \n",
    "    for chunk in initial_chunks:\n",
    "        if len(chunk.page_content) > 700:\n",
    "            sub_chunks = split_large_chunk(chunk, threshold=700)\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "            \n",
    "    chapter_chunks[chapter_name] = final_chunks\n",
    "    total_chunks += len(final_chunks)\n",
    "    \n",
    "    print(f\"  ✓ Final chunks: {len(final_chunks)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Chunking completed!\")\n",
    "print(f\"  Total chunks: {total_chunks}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: 체크포인트 저장 및 결과 확인\n",
    "\n",
    "분할된 청크를 체크포인트로 저장하고 샘플 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 4 저장\n",
    "checkpoint_4_chunks = chapter_chunks.copy()\n",
    "\n",
    "print(\"✓ Checkpoint 4 saved successfully!\")\n",
    "print(f\"  Variable: checkpoint_4_chunks\")\n",
    "print(f\"  Chapters: {list(checkpoint_4_chunks.keys())}\")\n",
    "\n",
    "# 샘플 청크 확인\n",
    "sample_chapter = \"Chapter 2\"\n",
    "if sample_chapter in checkpoint_4_chunks and checkpoint_4_chunks[sample_chapter]:\n",
    "    print(f\"\\n--- Sample: {sample_chapter} chunks ---\")\n",
    "    print(f\"Total chunks: {len(checkpoint_4_chunks[sample_chapter])}\")\n",
    "    \n",
    "    for i, chunk in enumerate(checkpoint_4_chunks[sample_chapter][:5]):\n",
    "        print(f\"\\n[Chunk {i+1}]\")\n",
    "        print(f\"  Metadata: {chunk.metadata}\")\n",
    "        print(f\"  Content length: {len(chunk.page_content)} chars\")\n",
    "        preview = chunk.page_content[:100].replace('\\n', ' ')\n",
    "        print(f\"  Preview: {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 5: Header-based Filtering\n",
    "\n",
    "특정 헤더를 가진 청크를 필터링하여 제거합니다.\n",
    "(예: Attribution, Exercises 등 학습에 불필요한 섹션)\n",
    "\n",
    "### 입력\n",
    "- `checkpoint_4_chunks`: 분할된 챕터별 청크\n",
    "- `filter_headers`: 제거할 헤더 키워드 리스트\n",
    "\n",
    "### 출력\n",
    "- `chapter_chunks_filtered`: 필터링된 챕터별 청크 (dict)\n",
    "\n",
    "### 체크포인트 저장\n",
    "- 변수: `checkpoint_5_filtered_chunks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: 필터링 설정\n",
    "\n",
    "학습에 불필요한 섹션을 제거하기 위한 설정을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제거할 헤더 키워드 설정\n",
    "filter_headers = header_labels  # 이전에 정의된 header_labels 사용\n",
    "\n",
    "# 각 챕터에서 강제로 제거할 초기 청크 수\n",
    "skip_first_chunks = 2\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Header-based Filtering Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Filter keywords: {len(filter_headers)}\")\n",
    "print(f\"  Skip first N chunks: {skip_first_chunks}\")\n",
    "\n",
    "for keyword in filter_headers:\n",
    "    print(f\"  - {keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: 필터링 함수 정의\n",
    "\n",
    "청크의 메타데이터를 확인하여 특정 헤더를 가진 청크를 필터링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_filter_chunk(chunk, filter_keywords):\n",
    "    \"\"\"\n",
    "    청크의 메타데이터 내 모든 헤더를 검사하여 필터링 대상인지 확인합니다.\n",
    "    \"\"\"\n",
    "    metadata = chunk.metadata\n",
    "    filter_keywords_lower = {k.lower() for k in filter_keywords}\n",
    "    \n",
    "    for key, value in metadata.items():\n",
    "        if key.startswith(\"Header\"):\n",
    "            if isinstance(value, str) and value.lower() in filter_keywords_lower:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "print(\"✓ Filtering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3: 챕터별 청크 필터링 실행\n",
    "\n",
    "1. 위치 기반 제거: 각 챕터의 처음 N개 청크 제거\n",
    "2. 키워드 기반 제거: 특정 헤더를 포함하는 청크 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_chunks_filtered = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Header-based Filtering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_original = 0\n",
    "total_filtered = 0\n",
    "\n",
    "for chapter_name, chunks in checkpoint_4_chunks.items():\n",
    "    print(f\"\\n[{chapter_name}]\")\n",
    "    print(f\"  Original: {len(chunks)} chunks\")\n",
    "    \n",
    "    # Step 1: 위치 기반 제거\n",
    "    remaining = chunks[skip_first_chunks:] if len(chunks) > skip_first_chunks else chunks\n",
    "    \n",
    "    # Step 2: 키워드 기반 필터링\n",
    "    filtered = [c for c in remaining if not should_filter_chunk(c, filter_headers)]\n",
    "    \n",
    "    chapter_chunks_filtered[chapter_name] = filtered\n",
    "    \n",
    "    total_original += len(chunks)\n",
    "    total_filtered += len(filtered)\n",
    "    \n",
    "    print(f\"  Filtered: {len(filtered)} chunks\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Filtering completed!\")\n",
    "print(f\"  Original: {total_original} → Filtered: {total_filtered}\")\n",
    "print(f\"  Retention: {total_filtered / total_original * 100:.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4: 체크포인트 저장 및 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 5 저장\n",
    "checkpoint_5_filtered_chunks = chapter_chunks_filtered.copy()\n",
    "\n",
    "print(\"✓ Checkpoint 5 saved successfully!\")\n",
    "print(f\"  Variable: checkpoint_5_filtered_chunks\")\n",
    "print(f\"  Chapters: {list(checkpoint_5_filtered_chunks.keys())}\")\n",
    "\n",
    "# 샘플 확인\n",
    "sample_chapter = \"Chapter 1\"\n",
    "if sample_chapter in checkpoint_5_filtered_chunks:\n",
    "    chunks = checkpoint_5_filtered_chunks[sample_chapter]\n",
    "    print(f\"\\n--- Sample: {sample_chapter} ({len(chunks)} chunks) ---\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"\\n[Chunk {i+1}]\")\n",
    "        print(f\"  Metadata: {chunk.metadata}\")\n",
    "        preview = chunk.page_content[:150].replace('\\n', ' ')\n",
    "        print(f\"  Preview: {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 6: Embedding with Clova\n",
    "\n",
    "각 청크에 대해 Naver Clova 임베딩 API를 호출하여 벡터를 생성합니다.\n",
    "QPM(Queries Per Minute) 제한을 고려하여 rate limiting을 적용합니다.\n",
    "\n",
    "### 입력\n",
    "- `checkpoint_5_filtered_chunks`: 필터링된 챕터별 청크\n",
    "- Clova API 설정 (환경변수)\n",
    "\n",
    "### 출력\n",
    "- `chunk_embeddings`: 청크별 임베딩 벡터와 메타데이터\n",
    "\n",
    "### 체크포인트 저장\n",
    "- 변수: `checkpoint_6_embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1: 환경변수 및 API 설정\n",
    "\n",
    ".env 파일에서 API 키를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Clova Studio Embedding v2 API 설정\n",
    "CLOVA_API_ENDPOINT = \"https://clovastudio.stream.ntruss.com/v1/api-tools/embedding/v2/\"\n",
    "CLOVA_API_KEY = os.getenv(\"CLOVASTUDIO_API_KEY\")\n",
    "\n",
    "# Rate Limiting 설정\n",
    "QPM_LIMIT = 60  # 분당 쿼리 제한\n",
    "\n",
    "print(\"Clova Studio Embedding v2 API Configuration:\")\n",
    "print(f\"  Endpoint: {CLOVA_API_ENDPOINT}\")\n",
    "print(f\"  QPM Limit: {QPM_LIMIT}\")\n",
    "print(f\"  API Key configured: {'Yes' if CLOVA_API_KEY else 'No (Please check .env)'}\")\n",
    "\n",
    "if not CLOVA_API_KEY:\n",
    "    raise ValueError(\"CLOVASTUDIO_API_KEY not found in environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Rate Limiter 구현\n",
    "\n",
    "QPM 제한을 준수하는 Rate Limiter를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, qpm_limit: int):\n",
    "        self.qpm_limit = qpm_limit\n",
    "        self.request_times = deque()\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        now = datetime.now()\n",
    "        one_minute_ago = now - timedelta(minutes=1)\n",
    "        \n",
    "        # 1분 이내의 요청만 유지\n",
    "        while self.request_times and self.request_times[0] < one_minute_ago:\n",
    "            self.request_times.popleft()\n",
    "        \n",
    "        # QPM 제한에 도달한 경우 대기\n",
    "        if len(self.request_times) >= self.qpm_limit:\n",
    "            oldest_request = self.request_times[0]\n",
    "            wait_until = oldest_request + timedelta(minutes=1)\n",
    "            wait_seconds = (wait_until - now).total_seconds()\n",
    "            \n",
    "            if wait_seconds > 0:\n",
    "                print(f\"    Rate limit reached. Waiting {wait_seconds:.1f}s...\")\n",
    "                time.sleep(wait_seconds + 0.1)\n",
    "                \n",
    "                now = datetime.now()\n",
    "                one_minute_ago = now - timedelta(minutes=1)\n",
    "                while self.request_times and self.request_times[0] < one_minute_ago:\n",
    "                    self.request_times.popleft()\n",
    "        \n",
    "        self.request_times.append(now)\n",
    "\n",
    "rate_limiter = RateLimiter(QPM_LIMIT)\n",
    "print(\"✓ Rate Limiter initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: Clova Embedding 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clova_embedding(text: str, retry_count: int = 3) -> List[float]:\n",
    "    \"\"\"\n",
    "    Clova Studio Embedding v2 API를 호출하여 임베딩을 생성합니다.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {CLOVA_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    request_body = {\"text\": text}\n",
    "    \n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            rate_limiter.wait_if_needed()\n",
    "            \n",
    "            response = requests.post(\n",
    "                CLOVA_API_ENDPOINT,\n",
    "                headers=headers,\n",
    "                json=request_body,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                embedding = result.get(\"result\", {}).get(\"embedding\", [])\n",
    "                if embedding:\n",
    "                    return embedding\n",
    "                raise Exception(\"No embedding in response\")\n",
    "            else:\n",
    "                print(f\"    API Error ({attempt + 1}/{retry_count}): {response.status_code}\")\n",
    "                if attempt < retry_count - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"    Exception ({attempt + 1}/{retry_count}): {str(e)[:100]}\")\n",
    "            if attempt < retry_count - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "    \n",
    "    raise Exception(f\"Failed to get embedding after {retry_count} attempts\")\n",
    "\n",
    "print(\"✓ Clova embedding function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.4: 청크별 임베딩 생성 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 모든 청크를 평탄화\n",
    "all_chunks = []\n",
    "for chapter_name, chunks in checkpoint_5_filtered_chunks.items():\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            'chapter': chapter_name,\n",
    "            'chunk_index': chunk_idx,\n",
    "            'metadata': chunk.metadata,\n",
    "            'content': chunk.page_content\n",
    "        })\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Clova Embedding Generation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "print(f\"Estimated time: ~{len(all_chunks) / QPM_LIMIT:.1f} minutes\")\n",
    "print()\n",
    "\n",
    "chunk_embeddings = []\n",
    "failed_chunks = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for idx, chunk_data in enumerate(all_chunks):\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"[{idx + 1}/{len(all_chunks)}] {chunk_data['chapter']}\")\n",
    "    \n",
    "    try:\n",
    "        embedding = get_clova_embedding(chunk_data['content'])\n",
    "        chunk_embeddings.append({\n",
    "            'chapter': chunk_data['chapter'],\n",
    "            'chunk_index': chunk_data['chunk_index'],\n",
    "            'metadata': chunk_data['metadata'],\n",
    "            'content': chunk_data['content'],\n",
    "            'embedding': embedding,\n",
    "            'embedding_dim': len(embedding)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {str(e)[:50]}\")\n",
    "        failed_chunks.append(chunk_data)\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"✓ Embedding completed!\")\n",
    "print(f\"  Success: {len(chunk_embeddings)}, Failed: {len(failed_chunks)}\")\n",
    "print(f\"  Total time: {total_time / 60:.1f} minutes\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.5: 체크포인트 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 체크포인트 6 저장\n",
    "checkpoint_6_embeddings = chunk_embeddings.copy()\n",
    "\n",
    "# JSON 파일로 저장\n",
    "embeddings_for_json = []\n",
    "for item in checkpoint_6_embeddings:\n",
    "    embeddings_for_json.append({\n",
    "        'chapter': item['chapter'],\n",
    "        'chunk_index': item['chunk_index'],\n",
    "        'metadata': item['metadata'],\n",
    "        'content': item['content'],\n",
    "        'embedding': item['embedding'],\n",
    "        'embedding_dim': item['embedding_dim']\n",
    "    })\n",
    "\n",
    "json_file_path = 'checkpoint_6_embeddings.json'\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(embeddings_for_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Checkpoint 6 saved to {json_file_path}\")\n",
    "print(f\"  Total embeddings: {len(checkpoint_6_embeddings)}\")\n",
    "\n",
    "if checkpoint_6_embeddings:\n",
    "    sample = checkpoint_6_embeddings[0]\n",
    "    print(f\"\\n--- Sample ---\")\n",
    "    print(f\"  Chapter: {sample['chapter']}\")\n",
    "    print(f\"  Embedding dim: {sample['embedding_dim']}\")\n",
    "    print(f\"  Embedding preview: {sample['embedding'][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Checkpoint 7: PostgreSQL pgvector Loading\n",
    "\n",
    "생성된 임베딩 벡터를 PostgreSQL의 pgvector extension을 사용하여 데이터베이스에 적재합니다.\n",
    "\n",
    "### 입력\n",
    "- `checkpoint_6_embeddings`: 청크별 임베딩 벡터와 메타데이터\n",
    "- PostgreSQL 연결 정보 (환경변수)\n",
    "\n",
    "### 출력\n",
    "- 데이터베이스에 적재 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.1: 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "import json\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2: PostgreSQL 연결 설정\n",
    "\n",
    ".env 파일에서 DB 연결 정보를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL 연결 정보 (환경변수에서 로드)\n",
    "PG_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "PG_PORT = int(os.getenv(\"DB_PORT\", \"5432\"))\n",
    "PG_DATABASE = os.getenv(\"DB_NAME\", \"mydb\")\n",
    "PG_USER = os.getenv(\"DB_USER\", \"myuser\")\n",
    "PG_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# 테이블 설정\n",
    "TABLE_NAME = \"document_embeddings\"\n",
    "CATEGORY = \"Network\"\n",
    "\n",
    "print(\"PostgreSQL Configuration:\")\n",
    "print(f\"  Host: {PG_HOST}\")\n",
    "print(f\"  Port: {PG_PORT}\")\n",
    "print(f\"  Database: {PG_DATABASE}\")\n",
    "print(f\"  User: {PG_USER}\")\n",
    "print(f\"  Password: {'*' * len(PG_PASSWORD) if PG_PASSWORD else 'Not set'}\")\n",
    "print(f\"  Table: {TABLE_NAME}\")\n",
    "\n",
    "if not PG_PASSWORD:\n",
    "    print(\"\\n⚠ Warning: DB_PASSWORD not found in environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.3: 테이블 스키마 생성\n",
    "\n",
    "pgvector extension과 테이블을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        database=PG_DATABASE,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"✓ Connected to PostgreSQL\")\n",
    "    \n",
    "    # pgvector extension 생성\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    print(\"✓ pgvector extension enabled\")\n",
    "    \n",
    "    # 테이블 존재 여부 확인\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_name = '{TABLE_NAME}'\n",
    "        );\n",
    "    \"\"\")\n",
    "    table_exists = cursor.fetchone()[0]\n",
    "    \n",
    "    if table_exists:\n",
    "        print(f\"⚠ Table '{TABLE_NAME}' exists. Truncating...\")\n",
    "        cursor.execute(f\"TRUNCATE TABLE {TABLE_NAME};\")\n",
    "    else:\n",
    "        print(f\"Creating table '{TABLE_NAME}'...\")\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE {TABLE_NAME} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            content TEXT NOT NULL,\n",
    "            category VARCHAR(255) NOT NULL,\n",
    "            embedding VECTOR(1024) NOT NULL,\n",
    "            tsvector TSVECTOR,\n",
    "            metadata JSONB,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # 인덱스 생성\n",
    "        cursor.execute(f\"\"\"\n",
    "            CREATE INDEX {TABLE_NAME}_embedding_idx \n",
    "            ON {TABLE_NAME} USING hnsw (embedding vector_cosine_ops);\n",
    "        \"\"\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            CREATE INDEX {TABLE_NAME}_tsvector_idx \n",
    "            ON {TABLE_NAME} USING gin (tsvector);\n",
    "        \"\"\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            CREATE INDEX {TABLE_NAME}_category_idx \n",
    "            ON {TABLE_NAME} (category);\n",
    "        \"\"\")\n",
    "        print(\"✓ Table and indexes created\")\n",
    "    \n",
    "    print(\"✓ Database setup completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Database setup failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.4: 배치 INSERT 실행\n",
    "\n",
    "임베딩 데이터를 100개씩 배치로 PostgreSQL에 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting Batch INSERT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total embeddings: {len(checkpoint_6_embeddings)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "insert_sql = f\"\"\"\n",
    "    INSERT INTO {TABLE_NAME} (content, category, embedding, tsvector, metadata)\n",
    "    VALUES (%s, %s, %s, to_tsvector('english', %s), %s);\n",
    "\"\"\"\n",
    "\n",
    "inserted_count = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    for batch_start in range(0, len(checkpoint_6_embeddings), BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(checkpoint_6_embeddings))\n",
    "        batch = checkpoint_6_embeddings[batch_start:batch_end]\n",
    "        \n",
    "        batch_num = (batch_start // BATCH_SIZE) + 1\n",
    "        print(f\"[Batch {batch_num}] Inserting rows {batch_start + 1} to {batch_end}...\")\n",
    "        \n",
    "        batch_data = []\n",
    "        for item in batch:\n",
    "            batch_data.append((\n",
    "                item['content'],\n",
    "                CATEGORY,\n",
    "                item['embedding'],\n",
    "                item['content'],\n",
    "                json.dumps(item['metadata'])\n",
    "            ))\n",
    "        \n",
    "        execute_batch(cursor, insert_sql, batch_data, page_size=BATCH_SIZE)\n",
    "        inserted_count += len(batch_data)\n",
    "        print(f\"  ✓ Inserted {len(batch_data)} rows\")\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    total_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"✓ INSERT completed!\")\n",
    "    print(f\"  Inserted: {inserted_count} rows\")\n",
    "    print(f\"  Time: {total_time:.1f}s\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ INSERT failed: {str(e)}\")\n",
    "    conn.rollback()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.5: 데이터 검증 및 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Data Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # 총 레코드 수\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME};\")\n",
    "    total_count = cursor.fetchone()[0]\n",
    "    print(f\"\\nTotal records: {total_count}\")\n",
    "    \n",
    "    # 카테고리별 통계\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT category, COUNT(*) \n",
    "        FROM {TABLE_NAME} GROUP BY category;\n",
    "    \"\"\")\n",
    "    for category, count in cursor.fetchall():\n",
    "        print(f\"  {category}: {count}\")\n",
    "    \n",
    "    # 샘플 데이터\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT id, LEFT(content, 80) as preview, category\n",
    "        FROM {TABLE_NAME} ORDER BY id LIMIT 3;\n",
    "    \"\"\")\n",
    "    print(\"\\n--- Sample Records ---\")\n",
    "    for record_id, preview, category in cursor.fetchall():\n",
    "        print(f\"[{record_id}] {category}: {preview}...\")\n",
    "    \n",
    "    # 테이블 크기\n",
    "    cursor.execute(f\"SELECT pg_size_pretty(pg_total_relation_size('{TABLE_NAME}'));\")\n",
    "    print(f\"\\nTable size: {cursor.fetchone()[0]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ Validation completed!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Validation failed: {str(e)}\")\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"\\n✓ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}